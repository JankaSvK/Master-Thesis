\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

In the last decades, we witnessed a massive jump in the amount of digital information that every person keeps. Looking back 20-30 years ago, people used to record only several hours of their lives to capture the most valuable moments. Now, according to available estimates it is more than 500 hours of multimedia data uploaded every minute only to YouTube \footnote{\href{https://www.statista.com/statistics/259477/hours-of-video-uploaded-to-youtube-every-minute/}{https://www.statista.com/statistics/259477/hours-of-video-uploaded-to-youtube-every-minute/}}. Decreasing prices and wide availability of the recording electronics (especially cell phones) are causing an increase in the amount of multimedia data taken every day. Also, it became more common to share a videos from day-to-day lives.

This significant increase in the volume of multimedia data places several new challenges, i.e. the need for effective search. This problem try to tackle not only the researches, but the initiative comes also from companies. Companies try to help their customers to organize a vast amount of multimedia data (e.g., Google Photos, YouTube). These in-company solutions help the companies to organise the data, but the options to filter the data may be limited when offered to a user.

Multimedia search may include several barriers, for example an exessive computability requirements. Even though we do not tackle this problem in this thesis, we investigate a solutions which could create a promising way to create an easy to use programs to search in videos.

We focus on the search scenario where a user searches for an exact known scene in a database. For example, the user recalls a memory, which corresponds to the searched scene. This task of searching for a previously seen scene is often reffered as a known-item search (KIS). We can further divide this scenario by different prior knowledge about the scene. For this thesis, we work only with visual KIS tasks, i.e., the image of the scene was previously seen.

For the past several years, researchers organize annual competitions in order to increase the interest in user-centered multimedia search. One, for example, is TRECVID \todo[inline]{(TODO: reference) {\color{blue} ten je zrovna spis na automatic search}}. The organisation of competitions enhanced the research in this area.

In this thesis, we tackle the KIS task with two different approaches: 
\begin{enumerate}
  \item Visual Search with a known position of the key objects in the scene.
  \item Traversing faces in the dataset.
\end{enumerate}

In the first approach, we focus on searching the scenes via only visual models together with the approximate position of key objects in the scene. Users can create a collage of images, which reminds them the searched scene and then browse through the ranked scenes looking for the match.

The second approach is an experimental test of possibility for visual traversing through a large dataset of faces. To present a user with a feasible amount of faces in one display, we tackle this challenge by organizing the faces into multilevel views.

Our goal is to examine those two approaches and provide user-friendly interfaces for the user to search through the dataset. During the evaluation, we focus on We evaluate the success rate of each the mentioned approach. We provide evaluations on hand created search queries and their success rates -- i.e., how many times was the searched scene in the top ranked results.

The following chapters start with the overview of Related Work on solving KIS task. Then we continue with the summarization of some key concepts used in the Preliminaries \todo[inline]{\color{blue} jako ze RW?}. Afterward, the key part of the thesis is presented in two chapters. The first one focuses on solving the KIS task based on the visual similarity together with spatial information. The second chapter focuses on traversing a hierarchical structure that organizes images of faces with respect to their similarity. Both chapters include evaluation sections. As the appendix, we include both Programmers and User documentation.
