\chapter{Content-based Image Retrieval}
\label{ch:preliminaries}
\label{ch:content_based}

Image retrieval and image indexing have been an active research field since the 1970s. In 1978 (\cite{tamura1978textural}), a group of researchers proposed a system for retrieving textures based on the example texture. Since then, a wide range of techniques for image retrieval were presented. Traditional approaches included manual annotation of the images by textual or numerical metadata. The user could then formulate a query against these annotations to retrieve relevant images. This approach is often referred to as Concept-based image retrieval or meta-data search.

There are several drawbacks to the textual or numerical annotations. First of all, extensive human annotations are often needed to provide rich data for filtering. Also, including spatial information of the objects takes more resources than only writing down present objects in the image. Furthermore, the images often include too many details (i.e., type, color, or shape of the objects), which may be impossible to comprehend by manual annotations.  The annotations may not even represent a stable truth. With a different annotator, the annotations may include different details/objects, which were perceived differently. When the user searches for an image, they have to know the exact terms the annotators used in order to be able to retrieve the images they want. As the last problem, we pose with human annotations is the scalability. As the amount of information increases every second, there is no human capability to hand process all the examples.

During the 1990s, content-based image retrieval (\acrshort{cbir}) emerged (trend study from \cite{datta2008image}). In the CBIR approach, the images are indexed by features directly derived from their visual content using automatic or semi-automatic image processing techniques. Such indexing lacks building blocks (for example, verbal description); on the other hand, it provides low-level feature information about the whole images or its regions. The attributes of images are complex functions of regions of the image or the whole image.

CBIR has received considerable research interest in the last decades. With the advancement in Deep Learning, a new pool of possible complex functions to describe the images emerged. In our approaches, we use pre-trained neural networks to extract features. Based on these features, we implemented and evaluated several approaches to the CBIR task.

Presented techniques focus on the \acrlong{kis} task. An alternative could be an Ad Hoc search, where the goal is to retrieve all relevant items to the query. Known-item search task instead works with retrieving a known item from the dataset.

In the following chapters, we continue with the specifics of the individual approaches we present. Here we formulate the task and the goal.


\section{Task Formulation}
\label{s:task_formulation_preliminaries}

The goal of the thesis is to propose a system that can search and retrieve images based on visual similarity.  

First we define universe $\mathcal{U}$ as the set of the all possible images:

$$
    \mathcal{U} = \bigcup_{h,w \in \mathbb{N}^2} \{0, 1, 2, \ldots, 255 \}^{h \times w \times 3}
$$

This definition follows the standard description of an image as a matrix $w \times h$ of pixels, where three color channels describe each pixel. We work over a dataset $D$ of such images, that we can define as $D \subset \mathcal{U}$.

The user is then asked to find a target image $t \in D$, that they have some knowledge of, for example, by (imperfectly) remembering the image or other detailed description. This task is called a known-item search.

Our system then serves as a search engine, aiming to provide the user with the most relevant results based on the user input. The system leverages the visual similarity between the images in the dataset $D$ (as in chapter \ref{ch:face_search}), or the similarity between user-provided example pictures and the images from the dataset $D$ (as in chapter \ref{ch:object_location}).

\section{Feature Space}

To search over a dataset of images $D$, we need a metric of how similar two items are. This similarity could be computed between the images directly, although it is more common to compare their descriptors (i.e., their projections by a descriptor extraction function). A descriptor extraction function is a function $f_e: \mathcal{U} \rightarrow \mathcal{F}$, where $\mathcal{F}$ is a feature space. Given a descriptor extraction function $f_e$, we compute for each image in the dataset $D$ a descriptor (or feature vector) $L_I = f_e(I)$, where $I \in \mathcal{U}, L_I \in \mathcal{F}$. In our work, we use various neural networks as our descriptor extraction functions. It is common for neural networks to produce the descriptors in the space of real numbers $\mathbb{R}^n$, and in our case, we work only with the descriptor spaces, which are isomorph to the $\mathbb{R}^n$.

\section{Distance Measures}
\label{s:distance_measures}

To compare how similar descriptors of two images are, we use a concept of distance. Similar descriptors have smaller distance and vice versa. We use definitions for a distance space and metric space from the book \cite{deza2009encyclopedia}.

\theoremstyle{definition}
\begin{definition}{Distance space}
$(\mathcal{F}, \delta )$ is a set $\mathcal{F}$ equipped
with a distance $\delta : \mathcal{F} \times \mathcal{F} \rightarrow \mathbb{R}^+_0 \text{ satisfying } \forall x, y \in \mathcal{F}: \delta(x, y) \geq 0 \text{ (nonnegativity), } \delta(x, y) = \delta(y, x) \text{ (symmetry) and } \delta(x, x) = 0.$
\end{definition}

\theoremstyle{definition}
\begin{definition}{Metric space}
$(\mathcal{F}, \delta)$ is a distance space, where $\delta$ additionaly satisfies $\forall x, y, z \in \mathcal{F} : \delta(x,z) \leq \delta(x,y) + \delta(y,z)$ (triangle inequality) and $\delta(x, y) = 0 \Rightarrow x = y$
\end{definition}

In the next chapters, we often compare two high-dimensional descriptors. These descriptors are often produced as an output of a neural network. Our feature space is $\mathbb{R}^n$, given $n$ as the number of features. We present distance functions over the space $\mathbb{R}^n$ later used in the thesis. Euclidean and Manhattan distance on $\mathbb{R}^n$ form metric spaces. Note that, cosine distance, deduced from cosine similarity, does not follow triangle inequality and forms only a distance space.

\subsection{Euclidean Distance}

For given $p, q \in \mathbb{R}^n$ we compute the distance as:
\begin{equation}
d_e({p},{q}) = \sqrt{\sum_{i=0}^{n-1} (p_i - q_i)^2}    
\end{equation}


\subsection{Manhattan Distance}

For given $p, q \in \mathbb{R}^n$ we compute the distance as:
\begin{equation}
d_m({p},{q}) = {\sum_{i=0}^{n-1} |p_i - q_i|}    
\end{equation}


\subsection{Cosine Similarity}

For given $p, q \in \mathbb{R}^n$ we compute the similarity as:
\begin{equation}
\text{similarity}({p},{q}) = \cos ({p},{q})= \frac{pq}{\|{p}\| \|{q}\|} = \frac{ \sum_{i=0}^{n-1}{p_i q_i} }{ \sqrt{\sum_{i=0}^{n-1}{p_i^2}} \sqrt{\sum_{i=0}^{n-1}{q_i^2}} }
\end{equation}

Since the cosine similarity is bounded by one, we use a transformation $d = 1 - s$ to obtain a distance.

\begin{equation}
    d_{cos}({\bf p},{\bf q}) = 1 - \text{similarity}({\bf p},{\bf q})
\end{equation}