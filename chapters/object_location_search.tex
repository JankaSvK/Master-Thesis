\chapter{Search by Object Location}
\label{ch:object_location}

\emph{You see a picture in your head. Your friend was standing on the beach, and there was a little sandcastle on the left. The sea behind beautifully reflected the sun, which was setting.}

We can imagine that at that particular moment we were shooting a video of the scenery. However, years later, with a vast set of videos in our collections, it may be impossible to re-watch every one of them to find that particular memory. Not all of us can visualize the memory, but those who can say they have an excellent visual memory \footnote{\url{https://en.wikipedia.org/wiki/Visual_memory}}. We present a solution that can be used to search in a dataset based on such recalls of the memory.

In this chapter, we elaborate solutions for known-item search tasks based on the visual description of the scene. The input characteristics are that we can recall how the objects looked (i.e., they were wearing a red hoodie) and their relative location in the shot (i.e., top left corner). With that information, we look at the match in the database for a described scene. We encapsulate the information in input and refer to it as a \emph{collage}. It means cutting images and placing them onto an empty canvas, where their placing also carries a piece of information. We show an example of such a query in figure \ref{fig:query_collage_comparison}. On the left, we can see a cat centered in the image behind the window. On the right, we can see a possible visualization of such memory. The canvas is grey on which we place a window, which reminded us of the original one. On the center, we place a similarly colored cat.

\begin{figure}
\centering

\begin{subfigure}[t]{0.45\textwidth}
\includegraphics[width=0.9\linewidth]{img/cat_on_window} 
\caption{Query}
\label{fig:searched_scene}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
\includegraphics[width=0.9\linewidth]{img/cat_on_window_collage}
\caption{One of the possible collage description for the query}
\label{fig:collage_example}
\end{subfigure}

\caption{Example of searched scene (query) with possible visual description by a collage.}
\label{fig:query_collage_comparison}
\end{figure}

While we were describing the scene, we used words for it. However, this chapter with presented solutions does not focus on the search based on the verbal description of the objects. However, it is one of the possible alternatives to handling the problem. Supporting verbal descriptions often require extensively annotated data to use to train neural networks, and even though the advancements in the research area, it still has a limited ability for extensively described characteristics. Our approach aims to avoid these limitations of vocabulary size. We believe that visually we can capture more diversity. Words often omit more specific information about the look or are hard to obtain by annotation systems, since the annotation tool may have never seen such combination before. For example, a single word for a human may represent a visually wide range of possibilities, based on clothes, age, and other attributes. We aim to avoid this bottleneck, and we proceed with the search based visual similarities.


In this chapter we utilise approaches for solving known-item search task based on \emph{collage}. Collage $C$ is a (unordered) set of $k$ pictures $C = \{\text{collage\_image}_i\} \text{for}\, i \in \{1, 2, \dots, k\} $.  Each $collage\_image_i$ contains two characteristics: image (i.e., visual description) and spatial information (location). To avoid any scaling and resolution issues, we represent location by a relative offset from the left, top, bottom and right. Therefore, all attributes of spatial information are in range $[0,1]$

This chapter will present three solutions incorporating pre-trained neural networks. To learn more about the networks, head back to chapter \ref{ss:pretrained_models}. We start by a short description of user-program interaction to understand the origin of the queries we evaluate. The chapter invites the ideas with their evaluation. We test a different set of hyperparameters and investigating their effect on the performance of the system. More information about the dataset is provided in section \ref{s:dataset}. To learn more about the user-interface, check the documentation in chapter \ref{ch:user_guide}. 


\section{User-program interaction}

The query is a \emph{collage} of one or multiple images. Each image also includes its relative position in the canvas.

We provide a user with a canvas where she can place, move, and resize the images in the \emph{collage}. Interactively, the set of results is showed (the most similar results). The user can then alternate the query for a new search, or to investigate the displayed results. Figure \ref{fig:query_collage_comparison} shows an example of the query -- the collage of two images (window and cat). 

\section{Solutions overview}

In the rest of the chapter, we present different approaches to this task, and we test different settings to obtain the best set of hyperparameters. In figure \ref{fig:processing_pipeline}, we can see an overview of the system. The top path is a processing path for the items in the database. The features are precomputed (offline) for all images in the database. We call a \emph{record} an image with its features. Each of the features may also be linked with additional information. There is one record per input image, but each record may contain multiple feature vectors describing it.

Similarly, for a given collage, we extract features by using the same model as for the database extraction. We do that for each of the images in the collage. Since we only need to annotate a couple of images, this can be done online on CPU for most neural networks. In this phase, we have an annotated dataset and extracted features from query images. Next, if the records contain multiple feature vectors, we may filter them to decrease the number of vectors we will compare. We call this process \emph{Relevant Features Extraction}. Next, based on the features from the database and the features from the collage, we compute their distances. For each query image, we compute the distance to each record in the database. This step creates a ranking of the database records for each input image in the collage. The last step is to merge these rankings. In the example image, a final merged ranking is computed based on the average distance for all query images.

We tune several factors in the described pipeline. In the following text, each step is described individually. Although, since they create a system as a whole, we evaluate them in the full environment, but always focusing on the particular part we test. For the next sections, we progressively build the pipeline while describing the particular solutions we used. We start with feature extraction strategies, and then we continue with the next steps, which are independent of feature extraction.

\section{Features extraction strategies}

In the following section, we present three feature extraction techniques. The feature extraction technique defines how we extract features for our dataset as well as for our queries. These obtained features will be later compared against each other in a ranking mechanism. We kick off with the baseline, where we do not use the location of the objects in the collage. Then we move on to a solution, which splits the image into regions and compares only to the regions related to the object query. Our last method is experimental, and it involves storing a higher-dimensional feature vector. This approach performs no strict pre-cutting. Instead, it works with the last convolutional block from the neural network.

\begin{figure}[p!]
    \centering
    \includegraphics[scale=0.9]{img/features_pipeline_rotated.png}
    \caption{Overview of processing pipeline}
    \label{fig:processing_pipeline}
\end{figure}


\subsection{Baseline -- Image representation}

Firstly, we developed a simple approach, which ignores the information about the position of the images in the collage. We set this approach as our baseline to see how much we improved a simple solution like this by adding more complexity. For each image in the dataset, we compute one feature vector by feeding it to the pre-trained convolutional neural network. The images in the dataset are rescaled (without preserving ratio) to fit the network's input dimensions (224x224).

In figure \ref{fig:mobilenet_whole_image}, we display the performance of such an approach on the annotated set of collages. We present the results on the MobileNetV2. We use MobileNetV2 due to its low computability needs and therefore offering quick annotations. It is widely used in the task, where we expect online or near online evaluation. We included a short description of the network in the Related Work (section \ref{ss:pretrained_models}).
\todo[inline]{Would be nice to add Resnet50}

\begin{figure}
\centering
\begin{boxedverbatim}
Database:
    - image: 1 feature vector (1 Dimensional)
Query:
    - query_image: 1 feature vector
    - compared to: each feature vector in the dataset
\end{boxedverbatim}
\caption{Overview of the Image representation approach}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{img/mobilenet_whole_image.png}
    \caption{Performance of MobileNetV2 on annotated collages}
    \label{fig:mobilenet_whole_image}
\end{figure}

\subsection{Splitting an Image into Regions}

Our goal in the next presented solutions is to use also the information about the objects' location. Our dataset contains many different images, with objects in different positions. Although, as we have seen in the previous section, comparison to the whole image worked well. Here we propose to include higher granularity and to split the image into multiple regions. That way, we can compare the query images only to the region, which is related (for example, if the tree is placed in the bottom right corner, we would compare only to the region from the bottom right corner). If we cut each image to the same regions, we will also be able to speed up the processing.

We define cutting into regions. Each record contains one image, which is visually divided into  $N \times M$ regions. Then for each region, we compute a feature vector via a pre-trained model. When the query comes, we will compare the query only to the regions from the record, which overlap.

\begin{figure}
\centering
\begin{boxedverbatim}
Database:
    - image:
        - Multiple regions:
            - region coordinates
            - feature vector (1 Dimensional)
Query:


    - query_images: features vectors from M
    - compared to: only to related crops from each image
\end{boxedverbatim}
\caption{Overview of the Regions' approach}
\end{figure}

\subsubsection{Regions Shape}

To use transfer learning for conventional CNN and avoid additional skewing in resizing, we limit the regions to the shape of squares. Since this input size of the square is one of the parameters, we aim to use the squares size the same as the size of the input of the network. Keeping square regions helps us avoid unnecessary scaling, which may be introduced when the length of the square's side does not match with the input for the network.

A second limitation we create is for the regions to cover the record image fully. Since we now know we need square regions that cover a full image, there might be no way to cut an image into the regions without overlapping. Therefore, we create enough regions to cover the image entirely and split the excess between the regions. This excess is evenly distributed over the regions, creating equal overlaps. We split based on the predefined number of regions since the number of regions results in the complexity's multiplicative coefficient.

Allowing overlaps plays another role in this solution. With the rigid frame without overlapping, it could happen that they would split an object into two parts. With overlaps, we know that some part is shared between both regions.

Our fixed parameters are input shape width $s$ and the number of regions we want to use $N \times M$ and image size $h, w$. We solve the task of choosing regions splits for one axis; the second is done equivalently. We know that the last region has to end with the edge of the image. Therefore the starting coordinate of the last region is $h - s$. We then split the remaining "space" over axis over $N-1$ regions equally. We call this distance $step$ since it says the distance between the regions. The starting coordinate $r_i$ of the $i$-th region in a given axis is:

\begin{align*}
step = (h - s) / (n - 1) \\
r_i = {i \, step\,\text{for}\,i \in \{0, 1, 2, \dots, N - 1T\}}
\end{align*}

With the condition on full coverage of the image (i.e., \(s N >= \text{h}\), and for $M$ respectively), we obtain full coverage of the image by the regions. Overlaps are evenly distributed over both axes. Although, with bigger sized regions, overlaps may occur between more than two regions. For example, if we split an image with width 180 into three regions with a width of 96 pixels, some areas of the image will be included in all three regions. This happens, when the $step <= 2 s$.

We evaluate the performance of the same network using a different number of regions. This experiment is shown in the figure \ref{fig:different_number_regions}. Even though the number of regions was almost doubled (from 8 to 15), we only saw a slight improvement in performance. In the figure \ref{fig:different_region_size} we show the effect of the regions size. We see that the best performing model worked with $2\times4$ regions with the input size of $128\times128$. For this particular setup, it was able to rank 90\% of the collages in the 8\% of the database.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{graphs/0c36458e4a7754f349e4dd02e823acc5f192f0aaa42647313045530525f3db19.pdf}
\caption{An experiment comparing the effect of the changing number of regions.}
\label{fig:different_number_regions}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{graphs/901175c0015f71987720d10953133afa566d88a09a6d7182a074859ff4e8409e.pdf}
\caption{An experiment comparing the effect of the changing regions size.}
\label{fig:different_region_size}
\end{figure}


\subsubsection{Choice of regions}

Previously, we defined a cutting into the regions for each item in the database. When the query comes, it is compared to the features for the given record. We ask a question if we should compare to all regions that overlap with the location of the query image. The reason is that we assume we have a query image that covers two-thirds of the canvas. In such a case, it probably overlapped almost or even all regions. Comparing the features with all of them is less effective than choosing only a subset of one of them.

Given one incoming query image with its position and shape, we propose several methods for choosing the "selected" regions. We visualize the problem in the figure \ref{fig:fish_with_grid}. The query would be an image of the fish placed within the red boundary. A green line displays all intercepted regions. The region with the highest "coverage" is the blue one.

One solution is to choose the region overlapping with the query the most (in the example, the blue one). To do that, we compute Intersection over Union (\ref{fig:intersection_over_union}) for each of the regions. This metric tells us which of the regions are more covered by the query and less.


We present an experiment, where we took all intercepted regions (green regions in the example), only the region with the highest IoU (blue region in the example), or equivalently first 2 with the highest IoU or first three regions. The results of the experiment are shown in figure \ref{fig:crop_limitation}. In the results, we see no significant improvement in any of the provided choices. Although, the selection of only the region with the highest overlap gives us the least computable heavy solution.

\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{img/fish_grid_regions}
\caption{Example of choosing the corresponding regions. Red: query position; Green: all intercepted regions; Blue: region with highest IoU.}
\label{fig:fish_with_grid}
\end{figure}


\begin{figure}
\centering
\includegraphics[width=\textwidth]{graphs/5c4a781f8e6f3eac93db2083bde3963c06582a92a8141411bf29e41251a98e75.pdf}
\caption{Performance of the system based on different number of chosen crops}
\label{fig:crop_limitation}
\end{figure}


\subsection{Using more information from antepenultimate layers from CNN}

\todo[color=yellow, inline]{DO NOT READ FURTHER}

The previously mentioned approach has a shortcoming. It is not able to grasp the arbitrary position of the object, rather it has limitations to previously fixed regions. This may lead to decreased performance for the objects, which cover a bigger part of the scene, i.e. in the previous case, it may overlap over multiple regions. This brings not only computational disadvantages but also reduces the amount of information which are used when compared to the query.

We propose an approach, where an antepenultimate layer of common CNN's may be used. Common CNNs are built out of several building blocks, i.e. convolution layers intervened by pooling (TODO image). This approach proved its ability for image recognition and many other related tasks (TODO add source). Many of the current state-of-the-art models use the pooling layer before fully connected layer for categorization.

We aim to leverage this antepenultimate layer, which contains not only visual features but also spatial information.

\begin{figure}
\centering
\begin{boxedverbatim}
Database:
    - image:
        - feature vector (3 Dimensional):
    - model M
Query:
    - query_images: features vectors from M, then average pool)
    - compared to: average pool over selected region for item 
                   in the database
\end{boxedverbatim}
\caption{Overview of the Spatial approach}
\end{figure}


\subsubsection{Choosing region of interest in the layer}

Layers before pooling on which we focus are 3 dimensional. First two dimensions contain spatial information, which is propagated from the previous layers. The third dimension contains visual information.

In order to obtain only a part of this layer we are interested in (i.e. our query was placed in that specific region) we need to take only a subset over first two dimensions. For a query defined by Qi = (y, x, h, w) and layer Li with first two dimensions H, W we consider a Li[y * H, x*W, (y+h) * H, (x + w) * W]. 

It is important to notice, that these layers use to have significantly lower height and width, compared to the input of the network. For example, before-last-pooling layer of Resnet50V2 has dimensions (7,7,2048). Therefore, we round our subset to nearest whole number and in case of no intersection we maintain at least intersection of size 1x1.

After this choice of the region of interest, we continue with the pooling layer in order to obtain one feature vector.

In comparison with the previous Splitting in regions approach, we see an enhanced ability to grasp more variability in object position. Especially, in queries which overlap most of the regions. On the contrary, this approach may require more memory, as for comparison for Resnet50V2 with 12 regions we needed to work only with 12 feature vectors. With the before-last-pooling layer we need for Resnet50V2 to work with 49 feature vectors. This may not be limitation with smaller feature vectors, but may come as a practical limitation based on the size of the dataset and computability power available.


\section{Ranking}
In the previous sections, we talked about the obtaining feature vector for the items in the database of the regions we are interested in. In this section, we take a closer look on further processing these obtained feature vectors.

Assuming we have an image for the query, we use the same model to compute the feature vector as was used to precompute the features for the database. Based on that we define a distance D, as the distance between the feature vector of the query and feature vector of the item in the database. This comparison to each database item gives us the distance between each item in the database and our query image.

Based on these distances we order the results, starting from the lowest to the highest distance. This distance acts as an inverse for the similarity. Similar to the results are, lower is the distance.

We use 3 main distance metrics:
Cosine Distance
Euclidean L2 distance
Euclidean L1 distance

\section{Multiple objects in the scene}

So far we talked only about handling queries consisting of one searched object. This limitation is very strict and therefore we experiment with multiple approaches to index the database based on multiple objects in the scene.

Firstly we make an assumption, that all query images have the same importance and are expected to be with the same level of relevance. Therefore we approach this problem as a set of query images, rather than as an ordered list.

We work further only with already precomputed rankings with corresponding distances for each image/part of the image. We propose several ways to merge the rankings.

We define ranking $R$ as a set of distances between the query image $q$ and database item $i$. We look for a function $r: R^n \rightarrow R$, which merges multiple rankings into one final ranking. Our goal is to minimize rank for each of the queries in the test items.

We tested several functions for this role and to compare a function that does not take into account the distances with different functions over the distances. A comparison for MobileNetV2 can be seen in figure \ref{fig:ranking_funcs}.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{graphs/362cb9a687ce05c7732f973defca88fb8c5c393f5992066521343314698c9de7}
\caption{Performance of the system based on different fusion method}
\label{fig:ranking_funcs}
\end{figure}

\section{Padding}

Our input images provided by a user do not have to be a square images. Although, we need to prepropress these to obey the input shape of the feature extraction model we use. These models use usually square shaped input. In our case, we compare the performance of the solution based on three different approaches on input reshaping. We compare padding the input with black or white and our third option is rescaling the images. Rescaling is done in the manner, where distortion occurs for non square images, i.e, a tree becomes shorter and wider, and the car becomes narrower and taller. The results are available in the figure \ref{fig:TODO}

\section{Dimensionality reduction}

In the previous sections we evaluated several hyperparameters of the system to achieve the best performance. In this section we take a look in reducing the dimensionality of the extracted features. The extracted features from the neural networks with removed last fully connected layer are often high dimenisonality data (for MobileNetV2 its 1024 features, for Resnet50 it is 2048 features). Even though we were able to test the hyperparameters without introducing another factor -- dimensionality reduction -- we want to test also if such reduction can have also positive effects.

\todo[inline]{Mozno viac popisat PCA?}

We perform Principal Component Analysis on the extracted features. We evaluate the effects of the PCA given different number of extracted components. This helps us to significantly reduce the size of the dataset used and therefore offers us a solution with good scalability.

