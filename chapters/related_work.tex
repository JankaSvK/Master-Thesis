\chapter{Related work}

%TODO: longer

This chapter reviews approaches made towards handling Known-item Search (KIS) task and the recent approaches for user-friendly traverse through the immersive amount of the visual data.

In recent years, we witnessed a significant advancement in the solution for the KIS task. The scale of the solution's complexity ranges from simple ones (e.g., sketching color blocks) with only a few descriptors to the ones which use recent advance in deep learning, as for concept labeling. 

The common simplification in this area is to replace videos with the set of images. This approach is simpler, as the user can easily see several key snapshots from the video compared to watching several minutes long video. (TODO: ref) propose a smart snapshotting of video instead of universal time sampling.

\section*{Known-item Search Task}
KIS task is one of the critical tasks for example in the Video Browser Showdown (TODO ref), which is taking place since 2012. We review few of the presented solutions to this task from the last VBS2019. We follow mainly the (TODO: ref)

The most common approach at the VBS2019 were "Query by an Image" and "Concept Labeling". Query by an image, in this case, mostly refers to finding the most similar results from databse to given image. The downside of this approach is mainly the difficulty to obtain an image, which would be sufficiently similar to the searched one.

The second most used approach at VBS2019 was Concept Labelling. In this case, a user can describe the scene using the words. The database is pre-annotated with the vocabulary of present items. During the request, the algorithm checks the database for the presence of the searched concept. This approach has a limitation of the vocabulary size. Recent advancement in the textual annotation neural networks is nowadays able to describe thousands of different objects by words, but still, this may present on the limitation on rarely used objects or hard to describe objects.

One of the approaches presented and used is by creating a color sketch. The user colors the canvas with respect to the original searched scene. The database is then traversed on the correspondence of the colors to the particular part of the image. We see a significant advantage of this approach to distinguish between the key objects in the scene spatially.
 
 Solving KIS task in VBS setting offers the option of using full video information and not only snapshots. This approach enhances the possibilities spectrum to Temporal Queries or Multimodal queries. Also, the solution included Optical Character Recognition (OCR) was presented. We present our solution as a possible enhancement of a complex system in order to create multiple search strategies based on user-preference. 



\section*{Traversion Approaches}

Since the KIS task is the task of two sides -- the algorithm and the user, it is essential to not forget about the easy to use interface. A good overview of the dataset may hide some deficiencies of the algorithm so that the user can still find the search scene. As the  (Evaluation of VBS ref) shows, the most common approach is to show a 2D grid of images to the user. Several approaches also provide an easy way to play the original video as one of the most immersive ways we have encountered to traverse a grid of images is placing these images on the globe with the possibility of multiway traversing (TODO: ref). We aim to achieve a similar level of smooth traversing.

The traversing systems rely on effective visualization techniques on high-number feature spaces. These systems create mostly a 2D grid of the images based on the distance between the samples in high dimensionality space. Though, this high dimensionality space, often produced by neural networks, may not have feasible representation in 2D space based on the hidden features. This is often caused by a lack of understanding and representability of the deep features. We aim to test this feature reduction to the 2D test in our face experiment with the use of deep neural network features.